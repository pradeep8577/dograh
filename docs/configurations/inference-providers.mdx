---
title: "Inference Provider"
description: "Dograh ships with its own inferencing engine, which is hosted at https://services.dograh.com. The inference service provides LLM, TTS and STT services. In this document you can see how you can configure the inferencing engine to your favourite provider, like OpenAI, Gemini etc."
---

## Configure Inference Provider

You can go to `https://app.dograh.com/model-configurations` if you are on hosted version of Dograh or go to `http://localhost:3010/model-configurations` if you are running Dograh locally.

You can see the configuration for the inference provider in the following screenshot.

![Model Configuration](../images/service-configuration.png)

You can select the provider from the dropdown and configure the API key, model, etc. You can see [API Keys](api-keys) documentation for instructions on how to create Service Keys to be used in Model Configuration.

## Next Steps

You can see how to configure the telephony provider in [Telephony Integrations](/telephony/twilio).
